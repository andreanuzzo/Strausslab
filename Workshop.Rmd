---
title: "Workshop"
output:
  html_notebook:
    fig_height: 9
    fig_width: 12
    theme: spacelab
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    fig_height: 9
    fig_width: 12
    theme: spacelab
    toc: yes
    toc_float: yes
---
#Set working directory
```{r setup}
library(knitr)

opts_knit$set(root.dir = normalizePath("/ufrc/strauss/andrea.nuzzo/projects/Workshop"))
```

#Visualizations
```{r}
library(tidyverse)
library(ggthemes)

plant_data <- read.csv('plant_data.csv', header = TRUE)

ggplot(plant_data, 
       aes(x=Biostimulant,
           y=Leaves/1000))+ #,fill=TimePoint)) +
  facet_grid(.~TimePoint) +
  geom_violin() +
  xlab('Biostimulants') +
  ylab('Leaves dry weight (g)') + theme_igray() +
  theme(axis.text.x  = element_text(angle=90, vjust=0.5)) +
  scale_fill_pander()

```


#Data Cleaning
```{r}
##Check NANs
colSums(is.na(plant_data))
```
```{r}
plant_data[!complete.cases(plant_data),]
```
##Imputing missing data
```{r}
##Imputing NANs with averages of subsets of the data (based on timepoint and biostimulants, thus taking the mean from replicates of the same missing sample)

plant_data$Area <- ifelse(is.na(plant_data$Area),
                      ave(plant_data[(plant_data$TimePoint==plant_data[which(is.na(plant_data$Area)),'TimePoint'] & plant_data$Biostimulant==plant_data[which(is.na(plant_data$Area)),'Biostimulant']),'Area'],
                                 FUN = function(x) mean(x, na.rm = TRUE)),plant_data$Area)

plant_data$Leaves <- ifelse(is.na(plant_data$Leaves),
                      ave(plant_data[(plant_data$TimePoint==plant_data[which(is.na(plant_data$Leaves)),'TimePoint'] & plant_data$Biostimulant==plant_data[which(is.na(plant_data$Leaves)),'Biostimulant']),'Leaves'],
                                 FUN = function(x) mean(x, na.rm = TRUE)),plant_data$Leaves)

##Check NANs
colSums(is.na(plant_data))
```
#Feature engineering
##Combine categorical variables
```{r}
lda_variables <- plant_data %>%
mutate(Biostimulant_TimePoint=paste(Biostimulant, TimePoint, sep = '_'))

```

##LDA Area vs TimePoint
```{r}
library(MASS)
area_vs_treatment_set = dplyr::select(plant_data, Area, TimePoint)
# Feature Scaling
area_vs_treatment_set[1] = scale(area_vs_treatment_set[1])
# Applying LDA
lda_area_vs_treatment = lda(formula = TimePoint ~ Area, 
                              data = area_vs_treatment_set)
area_vs_treatment_results = as.data.frame(predict(lda_area_vs_treatment, area_vs_treatment_set))
area_vs_treatment_results = area_vs_treatment_results[c(6, 1)]
names(area_vs_treatment_results) <- c('LDA1_area_vs_treatment')

# Check how much variance the LDA covers
lda_area_vs_treatment$svd^2/sum(lda_area_vs_treatment$svd^2)
```

```{r}
lda_variables <- cbind(lda_variables, area_vs_treatment_results[1])
```

##LDA Biomass vs Timepoint 
```{r}
biomass_vs_treatment_set = dplyr::select(plant_data, Leaves, Root, Stems, TimePoint)
# Feature Scaling
biomass_vs_treatment_set[-4] = scale(biomass_vs_treatment_set[-4])
# Applying LDA
lda_biomass_vs_treatment = lda(formula = TimePoint ~ Leaves+Root+Stems, 
                              data = biomass_vs_treatment_set)
biomass_vs_treatment_results = as.data.frame(predict(lda_biomass_vs_treatment, biomass_vs_treatment_set))
biomass_vs_treatment_results = biomass_vs_treatment_results[c(6,7,8,1)]
names(biomass_vs_treatment_results) <- c('LDA1_biomass_vs_treatment')
lda_biomass_vs_treatment$svd^2/sum(lda_biomass_vs_treatment$svd^2)
```

```{r}
lda_variables <- cbind(lda_variables, biomass_vs_treatment_results[1])
```
#Statistical exploratory analysis
##Correlation plots
```{r}
library(corrplot)
dummyvars <- model.matrix(X.SampleID~Biostimulant+Biostimulant_TimePoint,lda_variables)
encoded_vars <- cbind(dummyvars[,-1], lda_variables[,which(names(lda_variables)!='LinkerPrimerSequence' &
                                                  names(lda_variables)!='BarcodeSequence' &
                                                  names(lda_variables)!='Description' &
                                                  names(lda_variables)!='InputFileName' &
                                                  names(lda_variables)!='X.SampleID' &
                                                  names(lda_variables)!='Rep' &
                                                  names(lda_variables)!='Biostimulant' &
                                                  names(lda_variables)!='TimePoint' &  
                                                  names(lda_variables)!='Biostimulant_TimePoint')])
corr.mat = cor(encoded_vars, use='complete.obs')
p.mat = cor.mtest(encoded_vars)$p
row.names(p.mat) <- row.names(corr.mat)
colnames(p.mat) <- colnames(corr.mat)
corrplot(corr.mat, type='lower', 
         tl.pos = 'lt', 
         addCoefasPercent = TRUE, 
         method = 'square',
         number.cex = .4, tl.cex = 0.8,
         order = 'FPC',  tl.col="black",
         p.mat = p.mat, sig.level = 0.01, 
         insig = 'label_sig', 
         pch.cex = .3, 
         pch = '*') +
corrplot(p.mat, type = 'upper', add = T, tl.pos = 'n', method = 'circle', 
         col =rainbow(40, start = 5/6, end = 4/6), cl.lim =c(0,1), diag = FALSE)

```

##Tukey's HSD
```{r}
library(agricolae)

formula <- lda_variables %>%
  dplyr::select(-X.SampleID, -Biostimulant, -TimePoint, -Rep, -Biostimulant_TimePoint) %>%
  colnames(.) %>%
  paste(collapse = '+') %>%
  paste0("~Biostimulant*TimePoint")

model <- aov(formula = as.formula(formula),data = lda_variables)
HSD.test(model, c('Biostimulant','TimePoint'), group=TRUE, console = TRUE)
```
```{r}
readr::write_tsv(lda_variables,'mappings_LDA.txt')
```

#Phyloseq
##Import files

```{r}
library(phyloseq)
library(vegan)
library(ape)
library(dummies)

#File Paths
biom_path <- file.path('merged/biom/table_wo_chl_mit.biom')
tree_path <- file.path('merged/biom/tree.nwk')
DESEq2_path <- file.path('merged/biom/DESeq2_w_tax.biom')
map_path <- file.path('mappings_LDA.txt')

#Import to phyloseq table and merge into phyloseq objects
table <- import_biom(BIOMfilename = biom_path,
                      #refseqfilename = repseqfile,
                      parseFunction = parse_taxonomy_default, 
                      parallel = T)
tax_table(table) <-tax_table(table)[,1:7]
DESEq2.table <- import_biom(BIOMfilename = DESEq2_path,
                      #refseqfilename = repseqfile,
                      parseFunction = parse_taxonomy_default, 
                      parallel = T)
tax_table(DESEq2.table) <-tax_table(DESEq2.table)[,1:7]
metadata <- import_qiime_sample_data(map_path)
tree <- read_tree(tree_path)


phylobj <- merge_phyloseq(table, metadata, tree)
DESEq2.phylobj <- merge_phyloseq(DESEq2.table, metadata, tree)

#Adjust taxonomy names (to harmonize betwen UNITE and SILVA databases)
tax_table(phylobj) <- gsub(".*__", "", tax_table(phylobj))
colnames(tax_table(phylobj)) <- c("Kingdom", "Phylum", "Class", 
                    "Order", "Family", "Genus", "Species")

tax_table(DESEq2.phylobj) <- gsub(".*__", "", tax_table(DESEq2.phylobj))
colnames(tax_table(DESEq2.phylobj)) <- c("Kingdom", "Phylum", "Class", 
                    "Order", "Family", "Genus", "Species")

#Log-transform sample counts (if needed)
pslog <- transform_sample_counts(phylobj,function(x){log(1 + x)})
```

##Phylum barplots
```{r}
phyloglom <- tax_glom(phylobj, 'Phylum')

phylorel <- transform_sample_counts(phyloglom, function(x){100*x/sum(x)})

phylorel %>%
  filter_taxa(function(x){ mean(x) > 1}, TRUE) %>%
  psmelt()%>%
  group_by(Biostimulant, TimePoint, Phylum) %>%
  summarize(mean=mean(Abundance)) %>%
  ggplot() +
  aes(x=Biostimulant, y=mean, fill=Phylum, color=Phylum) +
  geom_bar(stat='identity') +
  facet_grid(.~TimePoint) +
  ylab('Relative abundance') +
  theme_igray() +
  theme(axis.text.x  = element_text(angle=90, vjust=0.5)) +
  scale_fill_pander() + scale_color_pander()
  
```

##Alpha Diveristy
```{r}
plot_richness(phylobj, 
              x = "Biostimulant", 
              color = "TimePoint", 
              measures = c('Observed', 'Chao1', 'Shannon')) + 
  geom_boxplot(alpha=.9) + theme_igray() +
  theme(axis.text.x  = element_text(angle=90, vjust=0.5))+
  scale_color_pander() + scale_fill_pander()
```
##Rarefaction plots
```{r}
source('https://raw.githubusercontent.com/mahendra-mariadassou/phyloseq-extended/master/R/richness.R')
ggrare(phylobj, step = 100, color = "Biostimulant", label = "Sample", se = FALSE) + 
  facet_wrap(~TimePoint) + guides(label=FALSE) + theme_igray() + scale_color_pander()
```

##PCoA with Bray distances
```{r}
PCoA.bray.ord<- ordinate(DESEq2.phylobj, "PCoA", distance = "bray")
PCoA.bray.plot <- plot_ordination(physeq = pslog,
                                        ordination = PCoA.bray.ord,
                                        color = "Biostimulant",
                                        shape = "TimePoint",
                                        title = "PCoA (Bray-Curtis distances)")
PCoA.bray.plot +geom_point(size=3) + scale_color_pander()
```
##PCoA with Unifrac distances
```{r}
PCoA.bray.ord<- ordinate(pslog, "PCoA", distance = "unifrac")
PCoA.bray.plot <- plot_ordination(physeq = pslog,
                                        ordination = PCoA.bray.ord,
                                        color = "Biostimulant",
                                        shape = "TimePoint",
                                        title = "PCoA (Unifrac distances)")
PCoA.bray.plot +geom_point(size=3) + scale_color_pander()
```
If you want to run PCoA with ALL possible distances (adapted from Phyloseq website https://joey711.github.io/phyloseq/distance.html) (requires a lot of memory)
```{r, eval=FALSE}
dist_methods <- unlist(distanceMethodList)
dist_methods = dist_methods[-which(dist_methods=="ANY")]

plist <- vector("list", length(dist_methods))
names(plist) = dist_methods
for( i in dist_methods ){
    # Calculate distance matrix
    iDist <- distance(pslog, method=i)
    # Calculate ordination
    iMDS  <- ordinate(pslog, "PCoA", distance=iDist)
    ## Make plot
    # Don't carry over previous plot (if error, p will be blank)
    p <- NULL
    # Create plot, store as temp variable, p
    p <- plot_ordination(pslog, iMDS, color="Biostimulant", shape="TimePoint")
    # Add title to each plot
    p <- p + ggtitle(paste("PCoA using distance method ", i, sep=""))
    # Save the graphic to file.
    plist[[i]] = p
}

df = ldply(plist, function(x) x$data)
names(df)[1] <- "distance"
p = ggplot(df, aes(Axis.1, Axis.2, color=Biostimulant, shape=TimePoint))
p = p + geom_point(size=3, alpha=0.5)
p = p + facet_wrap(~distance, scales="free")
p = p + ggtitle("PCoA on various distance metrics")
p

```

##NMDS
```{r}
NMDS_bray_b1 <- ordinate(DESEq2.phylobj, method = "NMDS", distance = "bray")
stressplot(NMDS_bray_b1)
plot_NMDS_bray_b1 <- plot_ordination(pslog,
                                  NMDS_bray_b1,
                                  type="samples",
                                  color="Biostimulant",
                                  shape="TimePoint")
plot_NMDS_bray_b1 + ggtitle("NMDS using Bray-Curtis distances") + scale_color_pander()
```
##PERMANOVA
```{r}
DESEq2.df = as(sample_data(DESEq2.phylobj), "data.frame")
DESEq2.distbray = distance(DESEq2.phylobj, "bray")
adonis(DESEq2.distbray ~ TimePoint*Biostimulant, DESEq2.df)
```
##ANOSIM and beta dispersivity
```{r}
attach(metadata)
NMDS_anosim = anosim(DESEq2.distbray, Biostimulant)
summary(NMDS_anosim)
plot(NMDS_anosim)
beta <- betadisper(DESEq2.distbray, DESEq2.df$Biostimulant) 
permutest(beta)
```

##CCA with VIF selection
```{r}
source('https://andreanuzzo.github.io/Strausslab/vif.cca.bw_sel.R')

vifvariables = lda_variables 

vifvariables = vifvariables[,which(names(vifvariables)!='LinkerPrimerSequence' &
                        names(vifvariables)!='BarcodeSequence' &
                        names(vifvariables)!='Description' &
                        names(vifvariables)!='InputFileName' &
                        names(vifvariables)!='X.SampleID' &
                        names(vifvariables)!='Batch' &
                        names(vifvariables)!='Rep')]

cca_vif <- vif.cca.bw_sel(DESEq2.phylobj, vifvariables, threshold = 5)
plot(cca_vif)
anova(cca_vif)
```
##CCA plot 
```{r}
cca_plot <- plot_ordination(physeq = pslog, 
                            ordination = cca_vif,
                            type= 'split',
                            color = "Biostimulant",
                            label = 'Phylum'
                            ) + aes(shape = TimePoint) + 
geom_point(aes(colour = Biostimulant))

cca_arrowmat <- scores(cca_vif, display = "bp") 
cca_arrowdf <- data.frame(labels = rownames(cca_arrowmat), cca_arrowmat)
cca_arrow_map <- aes(xend = CCA1, 
                 yend = CCA2, 
                 x = 0, 
                 y = 0, 
                 color = NULL,
                 shape = NULL)

cca_label_map <- aes(x = 1.3 * CCA1, 
                 y = 1.3 * CCA2, 
                 color = NULL, 
                 label = labels,
                 shape = NULL)

cca_arrowhead = arrow(length = unit(0.02, "npc"))
cca_plot + geom_segment(mapping = cca_arrow_map, size = .5, data = cca_arrowdf, 
               color = "black", arrow = cca_arrowhead) +
  geom_text(mapping = cca_label_map, size = 2, data = cca_arrowdf) +
  ggtitle("CCA with Bray-Curtis distances") + scale_color_pander()

```

##CAP Plot
```{r}
source('https://andreanuzzo.github.io/Strausslab/vif.cap.bw_sel.R')
cap_vif <- vif.cap.bw_sel(pslog, vifvariables, threshold = 5)

cap_plot <- plot_ordination(physeq = pslog, 
                            ordination = cap_vif,
                            type= 'split',
                            color = "Biostimulant", 
                            label = 'Phylum',
                            axes = c(1,2)) + 
  aes(shape = TimePoint) +
  geom_point(aes(colour = Biostimulant)) + scale_color_pander()
arrowmat <- scores(cap_vif, display = "bp") 
arrowdf <- data.frame(labels = rownames(arrowmat), arrowmat)
arrow_map <- aes(xend = CAP1, 
                 yend = CAP2, 
                 x = 0, 
                 y = 0, 
                 color = NULL,
                 shape = NULL)
label_map <- aes(x = 1.3 * CAP1, 
                 y = 1.3 * CAP2, 
                 color = NULL, 
                 label = labels,
                 shape = NULL)
arrowhead = arrow(length = unit(0.02, "npc"))
cap_plot + 
  geom_segment(mapping = arrow_map, size = .5, data = arrowdf, 
               color = "#0C3D4C", arrow = arrowhead) +
  geom_text(mapping = label_map, size = 2, data = arrowdf, show.legend = FALSE) +
  ggtitle("Constrained Analysis with Bray-Curtis distances") 
```

#Network analysis with SpiecEasi
##Import libraries
```{r message=FALSE, warning=FALSE}
library(SpiecEasi)
library(igraph)
library(indicspecies)
```

We will work on the log-normalized table that we produced in the merged pipeline, containing both fungal and bacterial features, and which has undergone a prefiltering process. Then  we get rid of all the zeroes in case there are some left. Then remove the taxa with zero abundance and we create a dummy taxon with the sum of the taxa (necessary for SpiecEasi). 

```{r warning=FALSE}

minocc = round(nrow(sample_data(DESEq2.phylobj))/6)
minocc_taxa <- filter_taxa(DESEq2.phylobj, function(x) sum(x > mean(x)) > (0.05*length(x)), TRUE)
extra_taxa <- setdiff(taxa_names(DESEq2.phylobj), taxa_names(minocc_taxa))
extra_taxa <- prune_taxa(extra_taxa, DESEq2.phylobj)

otus <- rbind(otu_table(minocc_taxa), taxa_sums(otu_table(extra_taxa)))
otus.f=as.matrix(otus)

taxa=tax_table(DESEq2.phylobj)
taxa.f=taxa[setdiff(1:nrow(taxa),taxa_names(extra_taxa)),]
dummyTaxonomy=c("k__dummy","p__","c__","o__","f__","g__","s__")
taxa.f=rbind(taxa.f,dummyTaxonomy)

rownames(taxa.f)[nrow(taxa.f)]="0"
rownames(otus.f)[nrow(otus.f)]="0"

updatedotus=otu_table(otus.f, taxa_are_rows = TRUE)
updatedtaxa=tax_table(taxa.f)

DESEq2.phylobj.f=phyloseq(updatedotus, updatedtaxa, metadata)
```

##Preliminary network
Now a first network (which will be randomly arranged at this stage), colored by Kingdom (takes a while)
```{r}
spiec.out=spiec.easi(DESEq2.phylobj.f, method="mb",icov.select.params=list(rep.num=10, ncores=3))
spiec.graph=adj2igraph(spiec.out$refit, vertex.attr=list(name=taxa_names(DESEq2.phylobj.f)))
plot_network(spiec.graph, DESEq2.phylobj.f, type='taxa', label=NULL, title = "Biomics", color = "Kingdom")
```

##Fast greedy algorithm
Now using the fast greedy algorithm to cluster the network. Greedy algorithms start by random points, and then increase the network without considering all the possible next points but taking the shortest path they encounter and then moving to the next point. This is quick, but it might lead to shorter paths than other algorithms. Once the clusters are individuated, then I show the counts of the phyla composing the 1st and 2nd biggest clusters.
```{r}
clusters=cluster_fast_greedy(spiec.graph)
clusterOneIndices=which(clusters$membership==1)
clusterOneOtus=clusters$names[clusterOneIndices]

clusterTwoIndices=which(clusters$membership==2)
clusterTwoOtus=clusters$names[clusterTwoIndices]

print("Phyla in cluster 1")
sort(table(taxa.f[clusterOneOtus,]),decreasing = TRUE)

print("Phyla in cluster 2")
sort(table(taxa.f[clusterTwoOtus,]),decreasing = TRUE)

```

Replot the network for a better visualization, colored by Phylum
```{r}
plot_network(spiec.graph, subset_taxa(DESEq2.phylobj.f, 
                                      Kingdom != "k__dummy" & 
                                      Kingdom != "Unassigned" & 
                                      Kingdom != "NA"), 
             type='taxa', color="Phylum", label=NULL, title = "Biomics")
```


##Directed network
Another option would be to plot the network by giving different colors to the edges according to the type of correlation. In this exercise, positive connections will give green edges, otherwise orange edges. (Note: the phyloseq plot_network function does not allow the coloring of the edges, thus I used the plot.igraph, which might get not as nice when coloring nodes by Phylum)
```{r}
#Construct the beta matrix (from spiec.easi output) and simmetrize
betaMat=as.matrix(symBeta(getOptBeta(spiec.out)))

#Define positive and negative interactions
positive=length(betaMat[betaMat>0])/2 
negative=length(betaMat[betaMat<0])/2 
total=length(betaMat[betaMat!=0])/2

print(paste("Out of", total, "edges, this network has", 
            positive, "positive edges and", 
            negative, "negative edges"))

#Define data
otu.ids=colnames(spiec.out$data)
edges=E(spiec.graph)
edge.colors=c()

#Function to assign per each edge a different color if positive or negative
for(e.index in 1:length(edges)){
  adj.nodes=ends(spiec.graph,edges[e.index])
  xindex=which(otu.ids==adj.nodes[1])
  yindex=which(otu.ids==adj.nodes[2])
  beta=betaMat[xindex,yindex]
  if(beta>0){
    edge.colors=append(edge.colors,categorical_pal(4)[3])
  }else if(beta<0){
    edge.colors=append(edge.colors,categorical_pal(4)[4])
  }
}

#Apply function
E(spiec.graph)$color=edge.colors
spiec.graph.b=spiec.graph

#Obtain kingdom nodes
E(spiec.graph.b)$arrow.size=5
vertexking=unique(tax_table(DESEq2.phylobj.f)[,1])
colbar <- categorical_pal(length(vertexking))

#Assign colors to the nodes by kingdom manually (I know...)
for(i in 1:length(components(spiec.graph.b)$membership)){
  OTU = names(components(spiec.graph.b)$membership[i])
  if(as.character(tax_table(DESEq2.phylobj.f)[OTU][,1]) == "Fungi"){
    V(spiec.graph.b)$color[i] <- colbar[1]
  } else if(as.character(tax_table(DESEq2.phylobj.f)[OTU][,1]) == "Bacteria"){ 
    V(spiec.graph.b)$color[i] <- colbar[2]
  } else {
    V(spiec.graph.b)$color[i] <- "black"
  }
}

#Plot
V(spiec.graph.b)$frame.color="black"
plot.igraph(spiec.graph.b, vertex.size=6, edge.width=3, vertex.label=NA, main="Biomics directed")
```

This is to save the files prior importing into Gephi. Before importing, remember to add for the spieceasi.X.csv file the first line which MUST be Source,Target,w (no quotes, no spaces); then add Id, before Kingdom in the X_lineages.csv. This will be your Node table file, while the other will be your Edge table file. Note that this will give UNDIRECTED networks (no positive or negative edges). 
```{r}
write.table(taxa.f,file=file.path("lineages.csv"), sep=",", quote=FALSE)
write.graph(spiec.graph,file=file.path("spieceasi.txt"),format="ncol") 
```

You will have to convert the csv file in a tab-separated file and remove the first line. Then, you will have to add "Id" to the first line of the `lineages.csv` file. _NOTE: This is only valid on MacOS!_
```{bash, eval=FALSE}
sed -i  '' 's/ /,/g' spieceasi.txt
sed -i  '' '1i\
Source,Target,w
' spieceasi.txt
sed -i  '' 's/Kingdom,Phylum,Class,Order,Family,Genus,Species/Id,Kingdom,Phylum,Class,Order,Family,Genus,Species/g' lineages.csv
```

##Fragility 
This is an example of a statistic you can derive from the network: fragility rate, which depends on the robustness, due to the connectivity. The network attack method prespecifies node order by hub characteristics (for example, degree and betweeness centrality) and assesses stability by natural connectivity (calculated as average eigenvalue of the graph adjacency matrix as the graph shrinks).
```{r}
#Define functions to get connectivity and eigenvalues
natcon <- function(ig) {
 N   <- vcount(ig)
 adj <- get.adjacency(ig)
 evals <- eigen(adj)$value
 nc  <- log(mean(exp(evals)))
 nc / (N - log(N))
}

#Define function to remove gradually nodes from the edges of the network
nc.attack <- function(ig) {
  hubord <- order(rank(betweenness(ig)), rank(degree(ig)), decreasing=TRUE)
  sapply(1:round(vcount(ig)*.8), function(i) {
    ind <- hubord[1:i]
    tmp <- delete_vertices(ig, V(ig)$name[ind])
    natcon(tmp)
  }) }

#Apply function
nc.ctl <- nc.attack(spiec.graph)

plot(seq(0,.8,len=length(nc.ctl)), nc.ctl, type='l', ylim=c(0,max(nc.ctl)), xlab="Proportion of removed nodes", ylab="natural connectivity")
hist(diff(nc.ctl), breaks=30, main="Fragility rates")
```

##Bipartite network. 
A Bipartite network uses fake nodes that can be whatever catergory you choose in your metadata, and plots the associations with the relevant taxa. Here, we will be using the code from the paper of Hartmann et al. 2018

```{r}
#Find indicator species
indic_otu <- as.data.frame(t(otus.f))
indic_groups <- sample_data(DESEq2.phylobj.f)$Biostimulant
length(unique(indic_groups))

set.seed(42)
indicatorsp <- multipatt(indic_otu,
                         indic_groups,
                         func = "r.g",
                         control=how(nperm=9999))
summary(indicatorsp, alpha=1,indvalcomp=T)

#Export indicator species
indicsp_df <- indicatorsp$sign
write.table(indicsp_df,"indicsp_df.txt",sep="\t",quote=F)

#Add the extra nodes
indsp.Control <- as.matrix(indicsp_df[which(indicsp_df$s.Control == 1 & 
                                              indicsp_df$p.value < 0.05),])
indsp.Endomaxx <- as.matrix(indicsp_df[which(indicsp_df$s.Endomaxx == 1 &
                                              indicsp_df$p.value < 0.05),])
indsp.Inocucor <- as.matrix(indicsp_df[which(indicsp_df$s.Inocucor == 1 &
                                              indicsp_df$p.value < 0.05),])
indsp.Pathway <- as.matrix(indicsp_df[which(indicsp_df$s.Pathway == 1 & 
                                              indicsp_df$p.value < 0.05),])
indsp.Sumagrow <- as.matrix(indicsp_df[which(indicsp_df$s.Sumagrow == 1 & 
                                               indicsp_df$p.value < 0.05),])

indsp <- rbind(indsp.Control, indsp.Endomaxx, indsp.Inocucor, indsp.Pathway, indsp.Sumagrow)

#Get an overview of the unique values
colnames(indsp)[1:5] <-levels(sample_data(DESEq2.phylobj.f)$Biostimulant)
range(indsp[,"stat"])
length(unique(rownames(indsp)))
length(unique(rownames(indsp)))/nrow(otus.f)

#Create the dataframe from which the bipartite network will be built
bipartite_df <- data.frame(from= c(rep("Control",length(which(indsp[,"Control"]==1))),
                                     rep("Endomaxx",length(which(indsp[,"Endomaxx"]==1))),
                                     rep("Inocucor",length(which(indsp[,"Inocucor"]==1))),
                                     rep("Pathway",length(which(indsp[,"Pathway"]==1))),
                                     rep("Sumagrow",length(which(indsp[,"Sumagrow"]==1)))),
                                 to= c(rownames(indsp)[which(indsp[,"Control"]==1)],
                                       rownames(indsp)[which(indsp[,"Endomaxx"]==1)],
                                       rownames(indsp)[which(indsp[,"Inocucor"]==1)],
                                       rownames(indsp)[which(indsp[,"Pathway"]==1)],
                                       rownames(indsp)[which(indsp[,"Sumagrow"]==1)]),
                                 r= c(indsp[which(indsp[,"Control"]==1),"stat"],
                                      indsp[which(indsp[,"Endomaxx"]==1),"stat"],
                                      indsp[which(indsp[,"Inocucor"]==1),"stat"],
                                      indsp[which(indsp[,"Pathway"]==1),"stat"],
                                      indsp[which(indsp[,"Sumagrow"]==1),"stat"]))
bipartite_attrib <- data.frame(node=unique(rownames(indsp)),indicgroup=0)

#Get which indicator species belongs to the indicator groups
for (i in as.character(bipartite_attrib$node)){
  bipartite_attrib[bipartite_attrib$node==i,"indicgroup"] <- paste(colnames(indsp)[which(indsp[i,1:5]==1)],collapse = "_")
}

bipartite_attrib <- cbind(bipartite_attrib,taxa.f[as.character(bipartite_attrib$node),])

## Create bipartite network with igraph
bi_graph <- graph.data.frame(bipartite_df, directed=T)

#Assign names to nodes (vertexes)
V(bi_graph)$type <- V(bi_graph)$name %in% bipartite_df[,1]
#Duda_bi_graph <- simplify(Duda_bi_graph, remove.multiple=T, remove.loops=T)

#Export for gephi, will require the same modifications as before
write.graph(bi_graph,file=file.path("bi_graph.csv"),format="ncol")
```

```{bash, eval=FALSE}
sed -i  '' 's/ /,/g' bi_graph.csv 
sed -i  '' '1i\
Source,Target,w
' bi_graph.csv
```

#Machine Learning in Qiime2
##Input preparation
```{bash, eval=FALSE}
#!/bin/bash

# ----------------SLURM Parameters----------------

#SBATCH -J q2_model_input
#SBATCH --time=1:00:00
#SBATCH --ntasks=1
#SBATCH -D /ufrc/strauss/andrea.nuzzo/projects/Workshop/merged
#SBATCH -o logs/4_prepare_input_%j.out
#SBATCH -A strauss

date;hostname;pwd

################################################################################
#
# This script will prepare the input table for the modelling 
# 
################################################################################

# ----------------Housekeeping--------------------
rm -r models
mkdir models
mkdir models/input
cd models

# ----------------Load Modules--------------------
module load qiime2

# ------------------Commands----------------------
qiime taxa filter-table \
  --i-table ../features/table.qza \
  --i-taxonomy ../features/taxonomy.qza \
  --p-exclude chloroplast,mithocondria \
  --o-filtered-table input/inptab.qza

cd input

#Check the merged table

qiime feature-table summarize  \
 --i-table inptab.qza  \
 --o-visualization table.qzv \
 --m-sample-metadata-file ../../../mapping_LDA.txt
 
 #Collapse taxa levels at genus level

 qiime taxa collapse \
 --i-table inptab.qza \
 --i-taxonomy ../../features/taxonomy.qza \
 --p-level 6 \
 --o-collapsed-table inptab_genus.qza

module unload qiime2

date

```

##Gneiss with OLS and LME
```{bash, eval=FALSE}
#!/bin/bash

# ----------------SLURM Parameters----------------

#SBATCH -J q2_gneiss
#SBATCH --mem=20g
#SBATCH --time=21:00:00
#SBATCH --ntasks=1
#SBATCH -n 8
#SBATCH -D /ufrc/strauss/andrea.nuzzo/projects/Workshop/merged
#SBATCH -o logs/5a_q2_gneiss_%j.out
#SBATCH -A strauss

date;hostname;pwd

################################################################################
#
# This script performs Gneiss log-transformed balances, then  OLS or LME modelling on your data
# 
################################################################################

# ----------------Housekeeping--------------------
cd models
rm -r gneiss
mkdir gneiss
cd gneiss

# ----------------Load Modules--------------------
module load qiime2

# ------------------Commands----------------------
 qiime feature-table filter-features \
   --i-table ../input/inptab.qza \
   --o-filtered-table filtered-table.qza \
   --p-min-samples 5

 qiime gneiss correlation-clustering \
   --i-table filtered-table.qza \
   --o-clustering hierarchy.qza

 qiime gneiss ilr-hierarchical \
   --i-table filtered-table.qza \
   --i-tree hierarchy.qza \
   --o-balances balances.qza

 qiime gneiss dendrogram-heatmap \
   --i-table filtered-table.qza \
   --i-tree hierarchy.qza \
   --m-metadata-file ../../../mappings_LDA.txt \
   --m-metadata-column Biostimulant \
   --p-color-map viridis \
   --o-visualization heatmap.qzv

 qiime gneiss ols-regression \
   --p-formula "TimePoint+Biostimulant+Biostimulant_TimePoint+Area+SPAD+Leaves+Stems+Root+Tot+LDA1_area_vs_treatment+LDA1_biomass_vs_treatment" \
   --i-table balances.qza \
   --i-tree hierarchy.qza \
   --m-metadata-file ../../../mappings_LDA.txt \
   --o-visualization regression_summary.qzv

 qiime gneiss lme-regression \
   --p-formula "Biostimulant+Area+SPAD+Leaves+Stems+Root+Tot+LDA1_area_vs_treatment+LDA1_biomass_vs_treatment" \
   --p-groups  TimePoint \
   --i-table balances.qza \
   --i-tree hierarchy.qza \
   --m-metadata-file ../../../mappings_LDA.txt \
   --o-visualization lme_all_vs_time.qzv
   
module unload qiime2

date

```
##Exctract Balances
###Option 1: one by one, qiime2 way

```{bash, eval=FALSE}
#!/bin/bash

# ----------------SLURM Parameters----------------

#SBATCH -J q2_balance_tax
#SBATCH --mem=10g
#SBATCH --time=1:00:00
#SBATCH --ntasks=1
#SBATCH -n 2
#SBATCH -D /ufrc/strauss/andrea.nuzzo/projects/Workshop/merged
#SBATCH -o logs/5b_balance_taxonomy_%j.out
#SBATCH -A strauss

date;hostname;pwd

################################################################################
#
# Finally, we want to extract the taxonomy for the balances which were 
# Significant in the previous steps
# 
################################################################################

# ----------------Housekeeping--------------------
cd models/gneiss
rm -r y*.qzv

# ----------------Load Modules--------------------
module load qiime2

# ------------------QUESTIONS---------------------
# Before going on, try to look at the summary of the model and answer: 
# How much is explained by the model? (R-squared)
# Does the model overfit? (pred_mse < model_mse)
# Do residuals show a trend? (if yes, you missed a variable)
# Are predicted in the residuals range? (Usually yes is expected for low R-squared)
# Do you have any significant balance in the OLS? (red in the heatmap)

# ------------------Commands----------------------
# If you have any significant balance and your model does not overfit, do the
# following for each balance to know who are the strains that significantly weigh
# for the chosen variable (i.e. Location)

qiime gneiss balance-taxonomy \
	  --i-table composition.qza \
	  --i-tree hierarchy.qza \
	  --i-taxonomy ../../features/taxonomy.qza \
	  --p-taxa-level 6 \
	  --p-balance-name y0 \
	  --m-metadata-file ../../../mappings_LDA.txt \
	  --m-metadata-column TimePoint \
	  --o-visualization y0_taxa_TimePoint.qzv 

qiime gneiss balance-taxonomy \
          --i-table composition.qza \
          --i-tree hierarchy.qza \
          --i-taxonomy ../../features/taxonomy.qza \
          --p-taxa-level 6 \
          --p-balance-name y0 \
          --m-metadata-file ../../../mappings_LDA.txt \
          --m-metadata-column Area \
          --o-visualization y0_taxa_Area.qzv
date

```

###Option 2: Do a batch extraction with a python script and then analize the mastodontic csv file
I wrote a small python script that can be called through a bash job. As long as you kept the folder structure consistent with the Qiime2 tutorial it will work on your files as well. The script is hosted on GitHub so you can download and modify it if needed.
```{bash, eval=FALSE}
#!/bin/bash

# ----------------SLURM Parameters----------------

#SBATCH -J q2_gneiss
#SBATCH --mem=20g
#SBATCH --time=21:00:00
#SBATCH --ntasks=1
#SBATCH -n 8
#SBATCH -D /ufrc/strauss/andrea.nuzzo/projects/Workshop/
#SBATCH -o merged/logs/5c_gneiss_extractor_%j.out
#SBATCH -A strauss

date;hostname;pwd

################################################################################
#
# Extraction of all balances having adjusted p-values<0.01 from the gneiss viz
#
################################################################################

module load qiime2/2018.6

wget -O merged/scripts/Balance_extractor.py https://andreanuzzo.github.io/Strausslab/Balance_extractor.py
merged/scripts/Balance_extractor.py

module purge
date
```

To elaborate the mastodontic file you had to use either R again or Tableau or similar. Excel will fail poorly. 
```{r}
rel.abund <- transform_sample_counts(phylobj, function(x){x/sum(x)})
```

```{r}
library(data.table)

extracted_balances <- fread('merged/models/gneiss/ols_summary_dir/extracted_balances.csv')
Area.balances <-extracted_balances %>%
  dplyr::select(kingdom, phylum, class, order, family, genus, species) %>%
  mutate_at(vars(kingdom, phylum, class, order, family, genus, species), funs(gsub(".*__", "",.)))

subset_taxa(rel.abund, Genus %in% Area.balances$genus &
            !Kingdom =='Unassigned' & !Kingdom =='Archaea' & !Kingdom =='Plantae') %>%
  psmelt() %>%
  group_by(TimePoint, Biostimulant, Kingdom, Phylum, Class, Area) %>%
  summarize(mean=mean(Abundance, na.rm = TRUE)*100) %>%
  filter(mean>1e-2) %>%
  mutate(bin=ntile(Area,10)*round(min(Area, na.rm = TRUE),0)) %>%
  ggplot() +
      aes(x=Biostimulant, y = mean, colour=Class, size=bin) +
      geom_point(position = position_jitter()) +
      facet_grid(Kingdom~TimePoint) +
      labs(y = "Relative Abundance %", x = "Area (sqcm)") + 
      theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5),
            legend.box = 'vertical') +
      guides(color=guide_legend(ncol=3)) +
      scale_colour_discrete(guide = guide_legend(title.position = "top", nrow = 1)) +
      scale_color_pander()+
      ggtitle('Classes differing for Area')
```


###Option 3: Jupyter notebook
The other option is to use the Qiime2 API and use a Jupyter notebook or an RStudio notebook to elaborate the balances. Requires a bit of python knowledge (you can do it!)
```{bash eval=FALSE}
nano merged/scripts/jupyter_notebook.sh
```

And then
```{bash, eval=FALSE}
#!/bin/bash

#----------SLURM Parameters-----------------#

#SBATCH --job-name=jupyter_andy
#SBATCH	-D /ufrc/strauss/andrea.nuzzo/Workshop/merged
#SBATCH --account=strauss
#SBATCH --output=logs/jupyter_%j.log
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=10gb
#SBATCH --time=72:00:00

date;hostname;pwd

module load qiime2/2018.6
 
jupyter-notebook --no-browser --port=23456 --ip='*'
 
date
```

```{bash eval=FALSE}
sbatch merged/scripts/jupyter_notebook.sh
```

Wait a couple of minutes. You will then have to open the log. You then need to open an ssh tunnel, using the address that the log gives you. It will appear as something like ```The Jupyter Notebook is running at:http://(c23a-s40.ufhpc or 127.0.0.1):23456/```. Copy the part before ufhpc and complete this command in a new terminal window. 

```{bash eval=FALSE}
ssh -NL 8000:c21b-s18.ufhpc:23456 your_email@hpg.rc.ufl.edu
```

You will be prompted with the request for password. After that, open a new Firefox window and digit ```localhost:8000``` to enter. If you didn't set your password, use the token that it's in the jupyter notebook log you open previously.



## Random Forest Analyses
```{bash, eval=FALSE}
#!/bin/bash

# ----------------SLURM Parameters----------------

#SBATCH -J q2_RF
#SBATCH --mem=30g
#SBATCH --time=21:00:00
#SBATCH --ntasks=1
#SBATCH -n 8
#SBATCH -D /ufrc/strauss/andrea.nuzzo/projects/Workshop/merged
#SBATCH -o logs/6_q2_RF_%j.out
#SBATCH -A strauss

date;hostname;pwd

################################################################################
#
# This script performs Regression/classification with RF on your data
# 
################################################################################
# ----------------Housekeeping--------------------
cd models
rm -r randomforest
mkdir randomforest
cd randomforest

# ----------------Load Modules--------------------
module load qiime2/2018.6

# ------------------Commands----------------------

#Classification on categorigal variables

qiime sample-classifier classify-samples \
    --i-table ../input/inptab_genus.qza \
    --m-metadata-file ../../../mappings_LDA.txt \
    --m-metadata-column TimePoint \
    --p-optimize-feature-selection \
    --p-parameter-tuning \
    --p-estimator RandomForestClassifier \
    --p-n-estimators 500 \
    --p-cv 5 \
    --p-random-state 42 \
    --p-n-jobs -1 \
    --p-palette GreenBlue \
    --output-dir RFC

qiime sample-classifier regress-samples \
  --i-table ../input/inptab_genus.qza \
  --m-metadata-file ../../../mappings_LDA.txt \
  --m-metadata-column Area \
  --p-optimize-feature-selection \
  --p-parameter-tuning \
  --p-estimator RandomForestRegressor \
  --p-n-estimators 500 \
  --p-cv 5 \
  --p-random-state 42 \
  --p-n-jobs -1 \
  --output-dir RFR

module unload qiime2
date

```
##Elaboration of random forest results
```{r}
rel.abund <- transform_sample_counts(phylobj, function(x){x/sum(x)})
```

```{r}
#These are the feature importances >1% in the RandomForest analysis made on qiime2
time.importance <-read.delim('merged/models/randomforest/Timepoint_feature_importance.tsv') %>%
  separate(feature, c("Kingdom", "Phylum", "Class", "Order", "Family","Genus"), sep=';') %>%
  mutate_at(vars(Kingdom, Phylum, Class, Order, Family, Genus), funs(gsub(".*__", "",.))) %>%
  filter(importance>1e-2)

subset_taxa(rel.abund, Genus %in% time.importance$Genus &
            !Kingdom =='Unassigned' & !Kingdom =='Archaea') %>%
  psmelt() %>%
  group_by(TimePoint, Biostimulant, Kingdom, Genus) %>%
  summarize(mean=mean(Abundance, na.rm = TRUE)*100, 
            stdev=sd(Abundance, na.rm = TRUE)) %>%
  ggplot() +
      aes(x=TimePoint, y = mean, fill=Genus) +
      geom_bar(stat = 'identity') +
      scale_fill_pander() + 
      facet_grid(Kingdom~Biostimulant) +
      theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
      labs(y = "Relative Abundance %", x = "Filtered Samples") + 
  ggtitle('Species differing over time')
```

```{r}
Area_RF <- read.delim('merged/models/randomforest/Area_feature_importance.tsv') %>%
  separate(feature, c("Kingdom", "Phylum", "Class", "Order", "Family","Genus"), sep=';') %>%
  mutate_at(vars(Kingdom, Phylum, Class, Order, Family, Genus), funs(gsub(".*__", "",.))) %>%
  filter(importance>1e-2)

subset_taxa(rel.abund, Genus %in% Area_RF$Genus & 
            !Kingdom =='Unassigned' & !Kingdom =='Archaea') %>%
  psmelt() %>%
  group_by(TimePoint, Area, Biostimulant, Kingdom, Genus) %>%
  summarize(mean=mean(Abundance, na.rm = TRUE), 
            stdev=sd(Abundance, na.rm = TRUE), 
            Area.mean = mean(Area, na.rm = TRUE),
            Area.stdev = sd(Area, na.rm = TRUE)) %>%
  write.csv('merged/models/randomforest/Area_RF.csv')
```
